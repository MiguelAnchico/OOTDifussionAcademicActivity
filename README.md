# OOTDiffusion Implementation Guide

Este repositorio contiene la implementaci√≥n y gu√≠a completa para instalar **OOTDiffusion** para virtual try-on, basado en el trabajo original de [levihsu/OOTDiffusion](https://github.com/levihsu/OOTDiffusion).

## üìã Tabla de Contenidos

- [Hardware Utilizado](#hardware-utilizado-en-las-pruebas)
- [Arquitectura del Modelo](#arquitectura-del-modelo)
- [Instalaci√≥n de OOTDiffusion](#instalaci√≥n-de-ootdiffusion)
- [Descarga de Modelos](#descarga-de-modelos)
- [Configuraci√≥n del Entorno](#configuraci√≥n-del-entorno)
- [Uso B√°sico](#uso-b√°sico)
- [Backend API Server](#backend-api-server)
- [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
- [Transferencia de Archivos](#transferencia-de-archivos)
- [Otros Modelos GAN](#otros-modelos-gan)
- [Cr√©ditos y Licencia](#cr√©ditos-y-licencia)

## üñ•Ô∏è Hardware Utilizado en las Pruebas

- **Entorno**: Cloud GPU con 80GB de almacenamiento
- **OS**: Linux
- **CUDA**: Disponible y compatible
- **Python**: 3.10
- **Tarjeta Gr√°fica (GPU)**:  RTX 3090 recomendado
- **GPU Memory**: M√≠nimo 12GB VRAM recomendado

## üèóÔ∏è Arquitectura del Modelo

OOTDiffusion utiliza una arquitectura de difusi√≥n avanzada que combina m√∫ltiples componentes para lograr un virtual try-on realista:

![Esquema OOTDiffusion](workflow_ootd.png)

### Componentes principales:
- **VAE Encoder/Decoder**: Codificaci√≥n y decodificaci√≥n de im√°genes
- **CLIP Image/Text Encoder**: Comprensi√≥n de im√°genes y texto
- **Outfitting UNet**: Red neuronal especializada en fusi√≥n de prendas
- **Denoising UNet**: Proceso de eliminaci√≥n de ruido en m√∫ltiples pasos
- **Mask Generator**: Generaci√≥n de m√°scaras para √°reas espec√≠ficas

El proceso combina la imagen de la persona, la prenda objetivo y opcionalmente etiquetas de categor√≠a para generar resultados precisos.

## üöÄ Instalaci√≥n de OOTDiffusion

### Paso 1: Configuraci√≥n inicial del entorno

```bash
# Verificar que tenemos CUDA disponible
nvidia-smi

# Actualizar el sistema
sudo apt update && sudo apt upgrade -y

# Instalar dependencias del sistema
sudo apt install -y git git-lfs build-essential cmake pkg-config libgl1-mesa-glx
```

### Paso 2: Clonar y configurar OOTDiffusion

```bash
# Clonar el repositorio
git clone https://github.com/levihsu/OOTDiffusion.git
cd OOTDiffusion

# Crear y activar entorno conda
conda create -n ootd python==3.10 -y
conda activate ootd

# Instalar PyTorch y dependencias espec√≠ficas
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2

# Instalar requirements
pip install -r requirements.txt
```

### Paso 3: Verificar instalaci√≥n inicial

```bash
# Verificar que todo est√° instalado correctamente
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

## üì¶ Descarga de Modelos

**Fuente de los modelos**: [levihsu/OOTDiffusion en Hugging Face](https://huggingface.co/levihsu/OOTDiffusion)

### Preparar estructura de directorios

```bash
# Instalar git-lfs
git lfs install

# Crear estructura de directorios
mkdir -p checkpoints/{ootd,humanparsing,openpose}
```

### Descargar modelos espec√≠ficos

```bash
# Modelos de human parsing
wget -O checkpoints/humanparsing/parsing_atr.onnx https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/humanparsing/parsing_atr.onnx
wget -O checkpoints/humanparsing/parsing_lip.onnx https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/humanparsing/parsing_lip.onnx
wget -O checkpoints/humanparsing/exp-schp-201908261155-lip.pth https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/humanparsing/exp-schp-201908261155-lip.pth

# Modelos principales de OOTD (pipelines completos)
git clone https://huggingface.co/levihsu/OOTDiffusion temp_ootd
cp -r temp_ootd/checkpoints/ootd checkpoints/
cp -r temp_ootd/checkpoints/openpose checkpoints/
rm -rf temp_ootd
```

### Archivos de modelos descargados

#### Human Parsing Models:

- `parsing_atr.onnx`: Modelo ONNX para human parsing ATR
- `parsing_lip.onnx`: Modelo ONNX para human parsing LIP
- `exp-schp-201908261155-lip.pth`: Modelo PyTorch para SCHP

#### OOTDiffusion Models:

- `checkpoints/ootd/`: Pipeline completo del modelo principal
- `checkpoints/openpose/`: Modelos para detecci√≥n de poses

#### CLIP Models:

- `clip-vit-large-patch14/`: Modelo CLIP para codificaci√≥n de im√°genes y texto

## ‚öôÔ∏è Configuraci√≥n del Entorno

### Soluci√≥n de incompatibilidades de versiones

Si encuentras el error de importaci√≥n con `huggingface_hub`, ejecuta:

```bash
# Aseg√∫rate de estar en el entorno correcto
conda activate ootd

# Instalar versi√≥n compatible
pip install huggingface_hub==0.19.4
pip install basicsr
```

### Verificar instalaciones

```bash
# Verificar versiones instaladas
pip list | grep -E "(huggingface|diffusers|transformers)"

# Verificar que las importaciones funcionen
python -c "from huggingface_hub import hf_hub_download; print('huggingface_hub OK')"
python -c "import diffusers; print('diffusers OK')"
```

## üéØ Uso B√°sico

### Preparar im√°genes de prueba

```bash
# Crear estructura para im√°genes de prueba
mkdir -p img_test/{clothe,person}

# Estructura esperada:
# img_test/
# ‚îú‚îÄ‚îÄ clothe/
# ‚îÇ   ‚îú‚îÄ‚îÄ 01260_00.jpg
# ‚îÇ   ‚îî‚îÄ‚îÄ 01430_00.jpg
# ‚îî‚îÄ‚îÄ person/
#     ‚îú‚îÄ‚îÄ 00891_00.jpg
#     ‚îî‚îÄ‚îÄ 03615_00.jpg
```

### Ejecutar OOTDiffusion

```bash
# Ir al directorio de ejecuci√≥n
cd OOTDiffusion/run

# Crear directorio de salida
mkdir -p images_output

# Half-body model (modelo de medio cuerpo)
python run_ootd.py \
    --model_path "../img_test/person/00891_00.jpg" \
    --cloth_path "../img_test/clothe/01260_00.jpg" \
    --model_type "hd" \
    --category "0" \
    --scale 2.0 \
    --sample 4

# Full-body model (modelo de cuerpo completo)
python run_ootd.py \
    --model_path "../img_test/person/00891_00.jpg" \
    --cloth_path "../img_test/clothe/01260_00.jpg" \
    --model_type "dc" \
    --category "1" \
    --scale 2.0 \
    --sample 4
```

### Par√°metros disponibles:

- `--model_type`: `"hd"` (half-body) o `"dc"` (full-body)
- `--category`: `"0"` (upper-body), `"1"` (lower-body), `"2"` (dresses)
- `--scale`: Factor de escala (1.0-3.0)
- `--sample`: N√∫mero de im√°genes a generar (1-4)

## üõ†Ô∏è Backend API Server

El proyecto incluye un servidor backend completo con detecci√≥n autom√°tica de personas y m√©tricas de rendimiento.

### Estructura del Backend

```
backend/
‚îú‚îÄ‚îÄ vto_server.py              # Servidor principal FastAPI
‚îú‚îÄ‚îÄ mobilenet_detector.py      # Detector de personas MobileNet-SSD
‚îú‚îÄ‚îÄ models/                    # Modelos de detecci√≥n
‚îú‚îÄ‚îÄ temp/                      # Archivos temporales
‚îî‚îÄ‚îÄ clothe/                    # Cat√°logo de prendas
```

### Instalaci√≥n del Backend

```bash
# Instalar dependencias adicionales del backend
pip install fastapi uvicorn python-multipart opencv-python

# Crear directorios necesarios
mkdir -p backend/{temp,clothe,models}

# Descargar modelo MobileNet-SSD
cd backend/models
wget https://github.com/chuanqi305/MobileNet-SSD/raw/master/mobilenet_iter_73000.caffemodel
wget https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/MobileNetSSD_deploy.prototxt
cd ../..
```

### Configurar Cat√°logo de Prendas

```bash
# Agregar prendas al cat√°logo
cp tu_ropa_1.jpg backend/clothe/shirt_1.jpg
cp tu_ropa_2.jpg backend/clothe/dress_2.jpg
cp tu_ropa_3.jpg backend/clothe/pants_3.jpg

# La detecci√≥n de categor√≠a es autom√°tica basada en el nombre del archivo
```

### Ejecutar el Servidor Backend

```bash
# Activar entorno
conda activate ootd

# Ir al directorio backend
cd backend

# Iniciar servidor
python vto_server.py
```

El servidor estar√° disponible en:
- **API**: `http://localhost:8384`
- **Documentaci√≥n**: `http://localhost:8384/docs`
- **M√©tricas**: `http://localhost:8384/metrics`

### Endpoints Disponibles

#### `POST /vto` - Virtual Try-On Principal
```bash
curl -X POST "http://localhost:8384/vto" \
     -H "Content-Type: multipart/form-data" \
     -F "person_image=@persona.jpg" \
     -F "clothe_id=1"
```

#### `GET /clothes` - Lista de Prendas Disponibles
```bash
curl "http://localhost:8384/clothes"
```

#### `GET /metrics` - M√©tricas de Rendimiento
```bash
curl "http://localhost:8384/metrics"
```

#### `GET /health` - Estado del Servidor
```bash
curl "http://localhost:8384/health"
```

### Respuesta de Ejemplo

```json
{
  "success": true,
  "image_url": "/result/out_hd_1234567890.png",
  "processing_time": 32.5,
  "clothe_id": 1,
  "clothe_name": "shirt_1.jpg",
  "category": "upperbody",
  "model_type": "hd",
  "metrics": {
    "average_time": 31.2,
    "category_average": 30.8,
    "recent_performance": {
      "count": 5,
      "average": 32.1,
      "min": 29.8,
      "max": 34.2
    }
  }
}
```

### Caracter√≠sticas del Backend

- **Detecci√≥n autom√°tica de personas** con MobileNet-SSD
- **Clasificaci√≥n autom√°tica de prendas** por nombre de archivo
- **M√©tricas de rendimiento** en tiempo real
- **Optimizaci√≥n de memoria GPU** con par√°metros reducidos
- **Gesti√≥n autom√°tica de archivos temporales**
- **API RESTful completa** con documentaci√≥n autom√°tica

## üé® Otros Modelos GAN

Este repositorio tambi√©n es compatible con otros modelos de virtual try-on basados en la arquitectura de OOTDiffusion:

- **VITON-HD**: Para im√°genes de alta resoluci√≥n
- **HR-VITON**: Modelo de alta resoluci√≥n con mejor calidad
- **CP-VTON**: Classic virtual try-on approach

### Configuraci√≥n de Modelos Adicionales

```bash
# Descargar modelos adicionales
mkdir -p checkpoints/viton-hd
wget -O checkpoints/viton-hd/model.pth [URL_DEL_MODELO]
```

## üìà Rendimiento y Optimizaci√≥n

### Configuraci√≥n Recomendada por Hardware

#### GPU de 8GB VRAM:
```bash
python run_ootd.py --scale 1.5 --sample 2
```

#### GPU de 16GB+ VRAM:
```bash
python run_ootd.py --scale 2.0 --sample 4
```

#### CPU Only (no recomendado):
```bash
export CUDA_VISIBLE_DEVICES=""
python run_ootd.py --scale 1.0 --sample 1
```

## üìù Cr√©ditos y Licencia

### Trabajo Original

Este proyecto est√° basado en **OOTDiffusion** desarrollado por:

- **Repositorio Original**: [levihsu/OOTDiffusion](https://github.com/levihsu/OOTDiffusion)
- **Paper**: "Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person"
- **Autores**: Yuhao Xu, Tao Gu, Weifeng Chen, Chengcai Chen
- **Hugging Face**: [levihsu/OOTDiffusion](https://huggingface.co/levihsu/OOTDiffusion)

### Implementaci√≥n del Backend

La implementaci√≥n del backend y servidor API es una extensi√≥n del trabajo original que a√±ade:

- API RESTful para integraci√≥n con aplicaciones web
- Sistema de m√©tricas y monitoreo de rendimiento  
- Detecci√≥n autom√°tica de personas con MobileNet-SSD
- Gesti√≥n autom√°tica de archivos y optimizaci√≥n de memoria

### Licencias

- **OOTDiffusion**: Sujeto a la licencia del repositorio original
- **Modelos Pre-entrenados**: Sujetos a sus respectivas licencias de Hugging Face
- **Extensiones del Backend**: MIT License

### Agradecimientos

Agradecemos especialmente a:

- **Equipo OOTDiffusion** por el desarrollo del modelo base y la investigaci√≥n
- **Hugging Face** por el hosting de modelos y facilidades de distribuci√≥n
- **Comunidad Open Source** por las contribuciones a las dependencias utilizadas

### Cita del Trabajo Original

Si utilizas este c√≥digo en investigaci√≥n acad√©mica, por favor cita el trabajo original:

```bibtex
@article{xu2024ootdiffusion,
  title={Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person},
  author={Xu, Yuhao and Gu, Tao and Chen, Weifeng and Chen, Chengcai},
  journal={arXiv preprint arXiv:2407.16224},
  year={2024}
}
```

---

**Implementaci√≥n con fines educativos y de investigaci√≥n basada en OOTDiffusion** üéìüëó
